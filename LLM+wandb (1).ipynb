{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b16098-bfe6-45f6-a615-0af0827a61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrbchwang\u001b[0m (\u001b[33mmrbchwang-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa08161-ee72-463f-ba01-b806b7bc5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a7334\\dl_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datasets import load_dataset #HUGGINGFACE\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e99b9-9998-4b64-b4e5-fa10085ce8a1",
   "metadata": {},
   "source": [
    "# config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f0dd44-7e53-4476-9f47-109b1ccb5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # 모델 아키텍처 (경량화)\n",
    "    'vocab_size': 50257,\n",
    "    'max_seq_length': 512,\n",
    "    'embedding_dim': 384,      # 작은 임베딩 차원\n",
    "    'num_heads': 8,            # 8개 헤드\n",
    "    'num_layers': 6,           # 6개 레이어 (가벼움)\n",
    "    'ff_dim': 1536,            # 4x embedding_dim\n",
    "    'hidden_dropout': 0.1,\n",
    "    'attention_dropout': 0.1,\n",
    "    \n",
    "    # 학습 설정\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # 데이터\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config': 'wikitext-103-v1',\n",
    "    'train_split': 'train',\n",
    "    'val_split': 'validation',\n",
    "    \n",
    "    # 기술\n",
    "    'use_amp': True,           # Mixed Precision Training\n",
    "    'use_flash_attention': False,  # Ampere 아키텍처 미지원\n",
    "    'use_gradient_checkpointing': True,\n",
    "    \n",
    "    # 체크포인트\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'save_total_limit': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72943f56-2eda-4489-8c1a-4156634f24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb(config: Dict):\n",
    "    wandb.init(\n",
    "        project='llm-rtx3060ti',\n",
    "        name='GPT-small-wikitext2',\n",
    "        config=config,\n",
    "    )\n",
    "    return wandb.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8df5da-9895-4683-a2b3-8ab85458f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Multi-Head Attention\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.output = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim) #d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        Q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim= -1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context = torch.matmul(attention_weights, V) #QK/d_k * V \n",
    "        context = context.transpose(1,2).contiguous()\n",
    "        context = context.view(batch_size, seq_length, self.embedding_dim)\n",
    "\n",
    "        output = self.output(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b15663-47d0-4c5f-851a-ac7ccdcd6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, ff_dim:int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embedding_dim)\n",
    "        self.activation = nn.ReLU() #RELU or GELU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95d9fce-c44b-4cd7-91ac-bbc0f04b29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ff_dim, dropout)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attention_output = self.attention(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attention_output)\n",
    "\n",
    "        ff_output = self.feed_forward(self.ln2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfac4671-20be-4edd-ae8a-edcdaca4297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config:Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.embedding_dim = config['embedding_dim']\n",
    "        self.max_seq_length = config['max_seq_length']\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        self.position_embedding = nn.Embedding(config['max_seq_length'], config['embedding_dim'])\n",
    "        self.embedding_dropout = nn.Dropout(config['hidden_dropout'])\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                config['embedding_dim'],\n",
    "                config['num_heads'],\n",
    "                config['ff_dim'],\n",
    "                config['hidden_dropout']\n",
    "            )\n",
    "            for _ in range(config['num_layers'])\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(config['embedding_dim'])\n",
    "        self.lm_head = nn.Linear(config['embedding_dim'], config['vocab_size'],bias=False)\n",
    "\n",
    "        #weight tying (sharing parameter)\n",
    "        self.lm_head.weight = self.token_embedding.weight \n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        pos_ids = torch.arange(seq_length, dtype=torch.long, device = input_ids.device)\n",
    "        pos_ids = pos_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        pos_embeds = self.position_embedding(pos_ids)\n",
    "        x = token_embeds + pos_embeds\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(seq_length, seq_length, device=input_ids.device))\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, causal_mask)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 100, \n",
    "                 temperature: float = 1.0, top_k: int = 50)->torch.Tensor:\n",
    "\n",
    "        device = input_ids.device\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            input_ids_cond = input_ids[:, -self.max_seq_length:]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self(input_ids_cond)\n",
    "\n",
    "            logits = logits[:, -1,:]/temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                top_k_vals, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits.scatter_(-1, top_k_indices, top_k_vals)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "        return input_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c614591f-9d13-44ee-9c7b-827e35b7af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,tokenized_data, block_size: int):\n",
    "        self.examples = []\n",
    "        self.block_size = block_size\n",
    "\n",
    "        all_tokens = []\n",
    "        for example in tokenized_data['input_ids']:\n",
    "            all_tokens.extend(example)\n",
    "\n",
    "        # block_size window sliding\n",
    "        ## divide all token sequence with block size\n",
    "        ### result : fixed size chunk\n",
    "        for i in range(0, len(all_tokens)-block_size, block_size):\n",
    "            self.examples.append(all_tokens[i:i+block_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.examples[idx]\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def load_and_tokenize_dataset(config: Dict, max_examples: int = None):\n",
    "    dataset = load_dataset(\n",
    "        config['dataset_name'],\n",
    "        config['dataset_config'],\n",
    "        split=config['train_split']\n",
    "    )\n",
    "\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(min(max_examples, len(dataset))))\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=config['max_seq_length'],\n",
    "            padding = 'max_length',\n",
    "            return_tensors=None,\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text'],\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8e935e-bd73-41c7-83f1-3912722805cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: Dict, device: torch.device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        self.scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Gradient scaler (AMP용)\n",
    "        if config['use_amp']:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Dict:\n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.config['num_epochs']}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, labels) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "                        # Trainer.train_epoch 첫 for 루프 안에 한번만 추가\n",
    "            if batch_idx == 0:\n",
    "                print(\"input_ids device:\", input_ids.device)\n",
    "                print(\"model first param:\", next(self.model.parameters()).device)\n",
    "\n",
    "            \n",
    "            # Forward pass with AMP\n",
    "            if self.config['use_amp']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = self.model(input_ids)\n",
    "                    loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                                      labels.reshape(-1))\n",
    "                \n",
    "                # Backward with scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                                  self.config['max_grad_norm'])\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "            else:\n",
    "                logits = self.model(input_ids)\n",
    "                loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                                  labels.reshape(-1))\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                                  self.config['max_grad_norm'])\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # 진행상황 업데이트\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # W&B 로깅\n",
    "            if self.global_step % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'train/loss': loss.item(),\n",
    "                    'train/learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "                    'train/epoch': epoch,\n",
    "                    'global_step': self.global_step,\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {'loss': avg_loss}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_loader: DataLoader, epoch: int) -> Dict:\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for input_ids, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            logits = self.model(input_ids)\n",
    "            loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                              labels.reshape(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        # W&B 로깅\n",
    "        wandb.log({\n",
    "            'val/loss': avg_loss,\n",
    "            'val/perplexity': perplexity,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'global_step': self.global_step,\n",
    "        }, path)\n",
    "        wandb.save(path)\n",
    "        print(f\"✓ Checkpoint saved: {path}\")\n",
    "    \n",
    "    def generate_samples(self, tokenizer, num_samples: int = 3) -> List[str]:\n",
    "        self.model.eval()\n",
    "        samples = []\n",
    "        \n",
    "        prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"In the beginning,\",\n",
    "            \"Machine learning is\",\n",
    "        ]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts[:num_samples]:\n",
    "                input_ids = torch.tensor(\n",
    "                    tokenizer.encode(prompt),\n",
    "                    dtype=torch.long\n",
    "                ).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                output_ids = self.model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.8,\n",
    "                    top_k=50\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(output_ids[0])\n",
    "                samples.append(generated_text)\n",
    "        \n",
    "        return samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b38da8e-1618-4402-ad5a-1bec09b68f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #wandb.login()\n",
    "    init_wandb(CONFIG)\n",
    "\n",
    "    tokenized_data, tokenizer = load_and_tokenize_dataset(CONFIG, max_examples=10000)\n",
    "    dataset = TextDataset(tokenized_data, CONFIG['max_seq_length'])\n",
    "\n",
    "    train_size = int(0.95*len(dataset))\n",
    "    val_size = len(dataset)-train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers =0,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers =0,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    model = GPTModel(CONFIG).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\" Model - Total params: {total_params/1e6:.2f}M, Trainable: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    trainer = Trainer(model,CONFIG, device)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        train_results = trainer.train_epoch(train_loader, epoch)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_results['loss']:.4f}\")\n",
    "\n",
    "        val_results = trainer.evaluate(val_loader, epoch)\n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {val_results['loss']:.4f}, Perplexity: {val_results['perplexity']:.2f}\")\n",
    "\n",
    "        samples = trainer.generate_samples(tokenizer, num_samples=3)\n",
    "        for i, sample in enumerate(samples):\n",
    "            wandb.log({f'sample_{i}': wandb.Html(f\"<p>{sample}<p>\")})\n",
    "\n",
    "        if val_results['loss'] < trainer.best_loss:\n",
    "            trainer.best_loss = val_results['loss']\n",
    "            trainer.save_checkpoint(f\"checkpoint_epoch_{epoch+1}_best.pt\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    trainer.save_checkpoint(\"checkpoint_final.pt\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f12dd8-2e3e-407c-abbe-2db7a61c9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffd1ece-1065-4939-b26f-0d02e1f32deb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\a7334\\wandb\\run-20260106_154615-x48cp11u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/x48cp11u' target=\"_blank\">GPT-small-wikitext2</a></strong> to <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/x48cp11u' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/x48cp11u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\a7334\\dl_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\a7334\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating test split: 100%|████████████████████████████████████████████| 4358/4358 [00:00<00:00, 120699.79 examples/s]\n",
      "Generating train split: 100%|████████████████████████████████████| 1801350/1801350 [00:00<00:00, 2014112.02 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████| 3760/3760 [00:00<00:00, 939956.08 examples/s]\n",
      "Map (num_proc=4): 100%|██████████████████████████████████████████████████| 10000/10000 [00:13<00:00, 766.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model - Total params: 30.14M, Trainable: 30.14M\n",
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████| 297/297 [13:13<00:00,  2.67s/it, loss=1.0613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.8490, Perplexity: 2.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_1_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:33<00:00,  2.54s/it, loss=0.5930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Loss: 0.7894, Perplexity: 2.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_2_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:33<00:00,  2.54s/it, loss=1.3228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.7219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Loss: 0.7564, Perplexity: 2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_3_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:33<00:00,  2.54s/it, loss=0.4355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.6803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Loss: 0.7334, Perplexity: 2.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_4_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:34<00:00,  2.54s/it, loss=0.5292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Loss: 0.7159, Perplexity: 2.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_5_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:31<00:00,  2.53s/it, loss=0.6580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.6148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Loss: 0.7063, Perplexity: 2.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_6_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:43<00:00,  2.57s/it, loss=0.5507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Loss: 0.6980, Perplexity: 2.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_7_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:36<00:00,  2.55s/it, loss=0.5094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.5689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Loss: 0.6945, Perplexity: 2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_8_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|                                                                              | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:37<00:00,  2.55s/it, loss=0.4848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.5549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Loss: 0.6921, Perplexity: 2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_9_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|                                                                             | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device: cuda:0\n",
      "model first param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████| 297/297 [12:42<00:00,  2.57s/it, loss=0.5581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Loss: 0.6914, Perplexity: 2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_10_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_final.pt\n",
      "Training completed\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▆▆▆▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▄█▆▇▇█▆▆█▅█▅▅▃▅▃▃▄▆▆▁▃▅▄▃▃▃▄▄</td></tr><tr><td>val/loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>val/perplexity</td><td>█▅▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>global_step</td><td>2900</td></tr><tr><td>train/epoch</td><td>9</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.56859</td></tr><tr><td>val/loss</td><td>0.69143</td></tr><tr><td>val/perplexity</td><td>1.99656</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT-small-wikitext2</strong> at: <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/x48cp11u' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/x48cp11u</a><br> View project at: <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti</a><br>Synced 4 W&B file(s), 30 media file(s), 0 artifact file(s) and 11 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260106_154615-x48cp11u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f19c626f-ea87-4018-ae8d-a1c48f339579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|█████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.35s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 0 ===\n",
      "[Prompt] The future of artificial intelligence\n",
      "[Generated] The future of artificial intelligence Finkelstein , Finkelstein has been used at the \" , \" ( Nix \" ) and \" a \" , which are based in the UK . \" It is an alternate published as a \" in The site in their \" ; it is\n",
      "\n",
      "=== Sample 1 ===\n",
      "[Prompt] In the beginning,\n",
      "[Generated] In the beginning,@ 000 people , The Importance of the novel that \" of the poem 's \" is a \" . The poem is the same day – the two of the poem , but is a great pleasure of the poem \" . He also the storyline of\n",
      "\n",
      "=== Sample 2 ===\n",
      "[Prompt] Machine learning is\n",
      "[Generated] Machine learning is the first of the same three of the two times of the film . This were a plan of the most example of the novel is used as the first to the book of a time of the film 's 's film the Church is a source of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(checkpoint_path: str, config: Dict):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 토크나이저 (학습 때와 동일)\n",
    "    _, tokenizer = load_and_tokenize_dataset(config, max_examples=10)  # 토크나이징은 안 써도 되니 소량만\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 모델 생성 후 체크포인트 로드\n",
    "    model = GPTModel(config)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_from_prompts(model, tokenizer, device, prompts, \n",
    "                          max_new_tokens: int = 50, \n",
    "                          temperature: float = 0.8, \n",
    "                          top_k: int = 50):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            input_ids = torch.tensor(\n",
    "                tokenizer.encode(prompt),\n",
    "                dtype=torch.long\n",
    "            ).unsqueeze(0).to(device)\n",
    "\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "\n",
    "            text = tokenizer.decode(output_ids[0])\n",
    "            results.append((prompt, text))\n",
    "    return results\n",
    "\n",
    "# 1) 사용할 체크포인트 경로 선택\n",
    "ckpt_path = \"checkpoint_final.pt\"        \n",
    "\n",
    "# 2) 모델 + 토크나이저 로드\n",
    "model, tokenizer, device = load_model_and_tokenizer(ckpt_path, CONFIG)\n",
    "\n",
    "# 3) 프롬프트 정의\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In the beginning,\",\n",
    "    \"Machine learning is\",\n",
    "]\n",
    "\n",
    "# 4) 문장 생성\n",
    "samples = generate_from_prompts(model, tokenizer, device, prompts,\n",
    "                                max_new_tokens=50,\n",
    "                                temperature=0.8,\n",
    "                                top_k=50)\n",
    "\n",
    "# 5) 출력\n",
    "for i, (prompt, text) in enumerate(samples):\n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(f\"[Prompt] {prompt}\")\n",
    "    print(f\"[Generated] {text}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5154cd8b-102a-4ec6-b27a-16941e4ebb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 0 ===\n",
      "[Prompt] South Korea is\n",
      "[Generated] South Korea is a wide for the second @-@ known as being the main scene of the British , as a variety of its \" long \" . The Crab between the group by the war of the war @-@ based on the Kingdom of its sixth commanding\n",
      "\n",
      "=== Sample 1 ===\n",
      "[Prompt] In the beginning,\n",
      "[Generated] In the beginning,@ 000 @,@ 000 copies were converted in response to find . The first time was reported to the airport had been to be held on the airport on the war , and the 20th century . In the war . The Irish language was the following\n",
      "\n",
      "=== Sample 2 ===\n",
      "[Prompt] Germany is\n",
      "[Generated] Germany is on a series of the world 's original original appearance at the 1960s for the UK critics , and in the United States videos since the United States . The United States is the city of the United States . A band is considered to the main Irish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"South Korea is\",\n",
    "    \"In the beginning,\",\n",
    "    \"Germany is\",\n",
    "]\n",
    "\n",
    "# 4) 문장 생성\n",
    "samples = generate_from_prompts(model, tokenizer, device, prompts,\n",
    "                                max_new_tokens=50,\n",
    "                                temperature=0.8,\n",
    "                                top_k=50)\n",
    "\n",
    "# 5) 출력\n",
    "for i, (prompt, text) in enumerate(samples):\n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(f\"[Prompt] {prompt}\")\n",
    "    print(f\"[Generated] {text}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (3.12 cu121)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
