{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b16098-bfe6-45f6-a615-0af0827a61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrbchwang\u001b[0m (\u001b[33mmrbchwang-hanyang-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa08161-ee72-463f-ba01-b806b7bc5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a7334\\dl_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datasets import load_dataset #HUGGINGFACE\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e99b9-9998-4b64-b4e5-fa10085ce8a1",
   "metadata": {},
   "source": [
    "# config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f0dd44-7e53-4476-9f47-109b1ccb5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # 모델 아키텍처 (경량화)\n",
    "    'vocab_size': 50257,\n",
    "    'max_seq_length': 512,\n",
    "    'embedding_dim': 384,      # 작은 임베딩 차원\n",
    "    'num_heads': 8,            # 8개 헤드\n",
    "    'num_layers': 6,           # 6개 레이어 (가벼움)\n",
    "    'ff_dim': 1536,            # 4x embedding_dim\n",
    "    'hidden_dropout': 0.1,\n",
    "    'attention_dropout': 0.1,\n",
    "    \n",
    "    # 학습 설정\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 7e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # 데이터\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config': 'wikitext-103-v1',\n",
    "    'train_split': 'train',\n",
    "    'val_split': 'validation',\n",
    "    \n",
    "    # 기술\n",
    "    'use_amp': True,           # Mixed Precision Training\n",
    "    'use_flash_attention': False,  # Ampere 아키텍처 미지원\n",
    "    'use_gradient_checkpointing': True,\n",
    "    \n",
    "    # 체크포인트\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'save_total_limit': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72943f56-2eda-4489-8c1a-4156634f24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb(config: Dict):\n",
    "    wandb.init(\n",
    "        project='llm-rtx3060ti',\n",
    "        name='GPT-RoPE-wikitext103',\n",
    "        config=config,\n",
    "    )\n",
    "    return wandb.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8df5da-9895-4683-a2b3-8ab85458f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Multi-Head Attention\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.output = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim) #d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        Q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim= -1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context = torch.matmul(attention_weights, V) #QK/d_k * V \n",
    "        context = context.transpose(1,2).contiguous()\n",
    "        context = context.view(batch_size, seq_length, self.embedding_dim)\n",
    "\n",
    "        output = self.output(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b15663-47d0-4c5f-851a-ac7ccdcd6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, ff_dim:int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embedding_dim)\n",
    "        self.activation = nn.GELU() #RELU or GELU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f478c08-1d89-459e-b262-795678734849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def build_rope_cache(seq_len: int, head_dim: int, device: torch.device):\n",
    "    \"\"\" RoPE\n",
    "        cos, sin\n",
    "        generating cache\"\"\"\n",
    "    theta = 1.0 / (10000 ** (torch.arange(0, head_dim,2, device=device).float()/ head_dim))\n",
    "    seq_idx = torch.arange(seq_len, device=device).float()\n",
    "    freqs = torch.einsum(\"i,j->ij\", seq_idx, theta)\n",
    "\n",
    "    cos = freqs.cos()\n",
    "    sin = freqs.sin()\n",
    "\n",
    "    cos = torch.stack([cos,cos], dim=-1).reshape(seq_len, -1)\n",
    "    sin = torch.stack([sin,sin], dim=-1).reshape(seq_len, -1)\n",
    "    return cos,sin\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos:torch.Tensor, sin:torch.Tensor):\n",
    "    cos = cos[None, None, :, :]\n",
    "    sin = sin[None, None, :, :]\n",
    "\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    x_rot = torch.stack([-x2,x1],dim= -1).reshape_as(x)\n",
    "\n",
    "    return x*cos + x_rot*sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0bb0e4-1770-45f7-8660-9c91244d1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Multi-Head Attention\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.output = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim) #d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        Q = self.query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        #apply RoPE\n",
    "        device = x.device\n",
    "        cos,sin = build_rope_cache(seq_length, self.head_dim, device)\n",
    "        Q = apply_rope(Q,cos, sin)\n",
    "        K = apply_rope(K, cos, sin)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim= -1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context = torch.matmul(attention_weights, V) #QK/d_k * V \n",
    "        context = context.transpose(1,2).contiguous()\n",
    "        context = context.view(batch_size, seq_length, self.embedding_dim)\n",
    "\n",
    "        output = self.output(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95d9fce-c44b-4cd7-91ac-bbc0f04b29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ff_dim, dropout)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attention_output = self.attention(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attention_output)\n",
    "\n",
    "        ff_output = self.feed_forward(self.ln2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfac4671-20be-4edd-ae8a-edcdaca4297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config:Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.embedding_dim = config['embedding_dim']\n",
    "        self.max_seq_length = config['max_seq_length']\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(config['hidden_dropout'])\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                config['embedding_dim'],\n",
    "                config['num_heads'],\n",
    "                config['ff_dim'],\n",
    "                config['hidden_dropout']\n",
    "            )\n",
    "            for _ in range(config['num_layers'])\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(config['embedding_dim'])\n",
    "        self.lm_head = nn.Linear(config['embedding_dim'], config['vocab_size'],bias=False)\n",
    "\n",
    "        #weight tying (sharing parameter)\n",
    "        self.lm_head.weight = self.token_embedding.weight \n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        x = token_embeds\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(seq_length, seq_length, device=input_ids.device))\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, causal_mask)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 100, \n",
    "                 temperature: float = 0.8, top_k: int = 50,\n",
    "                eos_token_id: int = 50256, repetition_penalty: float = 1.1)->torch.Tensor:\n",
    "\n",
    "        device = input_ids.device\n",
    "\n",
    "        for step in range(max_new_tokens):\n",
    "            input_ids_cond = input_ids[:, -self.max_seq_length:]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self(input_ids_cond)[:,-1,:]\n",
    "\n",
    "                for i, token_id in enumerate(input_ids[0, -50:]):\n",
    "                    logits[0,token_id] /= repetition_penalty ** (1.0/(i+1))\n",
    "\n",
    "                logits /= temperature\n",
    "    \n",
    "                if top_k > 0:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    threshold = v[:,[-1]]\n",
    "                    logits[logits < threshold] = float('-inf')\n",
    "    \n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "            if next_token_id.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "        return input_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c87f566-9dbf-4a72-828c-e2b976cc72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 50257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, CONFIG['vocab_size'], (2, 16))  # (batch, seq)\n",
    "model = GPTModel(CONFIG)\n",
    "logits = model(x)  # 에러 없이 (2,16,vocab) 나오면 OK\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c614591f-9d13-44ee-9c7b-827e35b7af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,tokenized_data, block_size: int):\n",
    "        self.examples = []\n",
    "        self.block_size = block_size\n",
    "\n",
    "        all_tokens = []\n",
    "        for example in tokenized_data['input_ids']:\n",
    "            all_tokens.extend(example)\n",
    "\n",
    "        # block_size window sliding\n",
    "        ## divide all token sequence with block size\n",
    "        ### result : fixed size chunk\n",
    "        for i in range(0, len(all_tokens)-block_size, block_size):\n",
    "            self.examples.append(all_tokens[i:i+block_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.examples[idx]\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def load_and_tokenize_dataset(config: Dict, max_examples: int = None):\n",
    "    dataset = load_dataset(\n",
    "        config['dataset_name'],\n",
    "        config['dataset_config'],\n",
    "        split=config['train_split']\n",
    "    )\n",
    "\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(min(max_examples, len(dataset))))\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=config['max_seq_length'],\n",
    "            padding = 'max_length',\n",
    "            return_tensors=None,\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text'],\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f8e935e-bd73-41c7-83f1-3912722805cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: Dict, device: torch.device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        self.scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Gradient scaler (AMP용)\n",
    "        if config['use_amp']:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Dict:\n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.config['num_epochs']}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, labels) in enumerate(pbar):\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            \n",
    "            # Forward pass with AMP\n",
    "            if self.config['use_amp']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = self.model(input_ids)\n",
    "                    loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                                      labels.reshape(-1))\n",
    "                \n",
    "                # Backward with scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                                  self.config['max_grad_norm'])\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "            else:\n",
    "                logits = self.model(input_ids)\n",
    "                loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                                  labels.reshape(-1))\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                                  self.config['max_grad_norm'])\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # 진행상황 업데이트\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # W&B 로깅\n",
    "            if self.global_step % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'train/loss': loss.item(),\n",
    "                    'train/learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "                    'train/epoch': epoch,\n",
    "                    'global_step': self.global_step,\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return {'loss': avg_loss}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_loader: DataLoader, epoch: int) -> Dict:\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for input_ids, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            logits = self.model(input_ids)\n",
    "            loss = self.loss_fn(logits.reshape(-1, self.config['vocab_size']),\n",
    "                              labels.reshape(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        # W&B 로깅\n",
    "        wandb.log({\n",
    "            'val/loss': avg_loss,\n",
    "            'val/perplexity': perplexity,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "        \n",
    "        return {'loss': avg_loss, 'perplexity': perplexity}\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'global_step': self.global_step,\n",
    "        }, path)\n",
    "        wandb.save(path)\n",
    "        print(f\"✓ Checkpoint saved: {path}\")\n",
    "    \n",
    "    def generate_samples(self, tokenizer, num_samples: int = 3) -> List[str]:\n",
    "        self.model.eval()\n",
    "        samples = []\n",
    "        \n",
    "        prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"In the beginning,\",\n",
    "            \"Machine learning is\",\n",
    "        ]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts[:num_samples]:\n",
    "                input_ids = torch.tensor(\n",
    "                    tokenizer.encode(prompt),\n",
    "                    dtype=torch.long\n",
    "                ).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                output_ids = self.model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=50,\n",
    "                    temperature=0.8,\n",
    "                    top_k=50\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(output_ids[0])\n",
    "                samples.append(generated_text)\n",
    "        \n",
    "        return samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b38da8e-1618-4402-ad5a-1bec09b68f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #wandb.login()\n",
    "    init_wandb(CONFIG)\n",
    "\n",
    "    tokenized_data, tokenizer = load_and_tokenize_dataset(CONFIG, max_examples=10000)\n",
    "    dataset = TextDataset(tokenized_data, CONFIG['max_seq_length'])\n",
    "\n",
    "    train_size = int(0.95*len(dataset))\n",
    "    val_size = len(dataset)-train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers =0,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers =0,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    model = GPTModel(CONFIG).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\" Model - Total params: {total_params/1e6:.2f}M, Trainable: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    trainer = Trainer(model,CONFIG, device)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        train_results = trainer.train_epoch(train_loader, epoch)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_results['loss']:.4f}\")\n",
    "\n",
    "        val_results = trainer.evaluate(val_loader, epoch)\n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {val_results['loss']:.4f}, Perplexity: {val_results['perplexity']:.2f}\")\n",
    "\n",
    "        samples = trainer.generate_samples(tokenizer, num_samples=3)\n",
    "        for i, sample in enumerate(samples):\n",
    "            wandb.log({f'sample_{i}': wandb.Html(f\"<p>{sample}<p>\")})\n",
    "\n",
    "        if val_results['loss'] < trainer.best_loss:\n",
    "            trainer.best_loss = val_results['loss']\n",
    "            trainer.save_checkpoint(f\"checkpoint_epoch_{epoch+1}_best.pt\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    trainer.save_checkpoint(\"checkpoint_final.pt\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ffd1ece-1065-4939-b26f-0d02e1f32deb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\a7334\\wandb\\run-20260111_145004-bnv4uhbx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/bnv4uhbx' target=\"_blank\">GPT-RoPE-wikitext103</a></strong> to <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/bnv4uhbx' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/bnv4uhbx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model - Total params: 29.95M, Trainable: 29.95M\n",
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:43<00:00,  2.57s/it, loss=0.8445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.8043, Perplexity: 2.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_1_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:38<00:00,  2.55s/it, loss=0.2645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.7487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Loss: 0.7326, Perplexity: 2.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_2_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:38<00:00,  2.56s/it, loss=0.7531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.6797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Loss: 0.6932, Perplexity: 2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_3_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:39<00:00,  2.56s/it, loss=0.6083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Loss: 0.6695, Perplexity: 1.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_4_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:39<00:00,  2.56s/it, loss=0.4883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Loss: 0.6532, Perplexity: 1.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_5_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:38<00:00,  2.56s/it, loss=0.5008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.5426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Loss: 0.6429, Perplexity: 1.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_6_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:38<00:00,  2.56s/it, loss=0.4254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Loss: 0.6390, Perplexity: 1.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_7_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:38<00:00,  2.56s/it, loss=0.3515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.4812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Loss: 0.6362, Perplexity: 1.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_epoch_8_best.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████| 297/297 [12:39<00:00,  2.56s/it, loss=0.4582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.4617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Loss: 0.6363, Perplexity: 1.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████| 297/297 [12:39<00:00,  2.56s/it, loss=0.4576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Loss: 0.6362, Perplexity: 1.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Linked 1 file into the W&B run directory (hardlinks); call wandb.save again to sync new files.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: checkpoint_final.pt\n",
      "Training completed\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▆▆▆▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅█▅▅▄▃▃▅▄▄▇▇▄▂▃▄▄▃▁▁▄▃▄▁▃▃▄</td></tr><tr><td>val/loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>val/perplexity</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>global_step</td><td>2900</td></tr><tr><td>train/epoch</td><td>9</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.59225</td></tr><tr><td>val/loss</td><td>0.63619</td></tr><tr><td>val/perplexity</td><td>1.88926</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT-RoPE-wikitext103</strong> at: <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/bnv4uhbx' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti/runs/bnv4uhbx</a><br> View project at: <a href='https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti' target=\"_blank\">https://wandb.ai/mrbchwang-hanyang-university/llm-rtx3060ti</a><br>Synced 4 W&B file(s), 30 media file(s), 0 artifact file(s) and 9 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260111_145004-bnv4uhbx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f19c626f-ea87-4018-ae8d-a1c48f339579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 0 ===\n",
      "[Prompt] The future of artificial intelligence\n",
      "[Generated] The future of artificial intelligence , the Egyptians called the gods who were to be a tradition of deities and was not known as the gods 's ba . The gods were the deities of their respective deities in other deities , and the gods were said to increase the gods . The gods\n",
      "\n",
      "=== Sample 1 ===\n",
      "[Prompt] In the beginning,\n",
      "[Generated] In the beginning, Baltimore Pike was later ordered to a new ship officer , but no effect of the rear ship attacked and replaced the remainder of the line . The ship was appointed as a major ship and was a well @-@ up area at Mahé 's junction\n",
      "\n",
      "=== Sample 2 ===\n",
      "[Prompt] Machine learning is\n",
      "[Generated] Machine learning is a very useful , but in a cell , are said to be a mildly image of a person 's desire , but there is a well @-@ known woman . Although the male is usually influenced by its bones , the wolf is revealed that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(checkpoint_path: str, config: Dict):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 토크나이저 (학습 때와 동일)\n",
    "    _, tokenizer = load_and_tokenize_dataset(config, max_examples=10)  # 토크나이징은 안 써도 되니 소량만\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 모델 생성 후 체크포인트 로드\n",
    "    model = GPTModel(config)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_from_prompts(model, tokenizer, device, prompts, \n",
    "                          max_new_tokens: int = 50, \n",
    "                          temperature: float = 0.8, \n",
    "                          top_k: int = 50):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            input_ids = torch.tensor(\n",
    "                tokenizer.encode(prompt),\n",
    "                dtype=torch.long\n",
    "            ).unsqueeze(0).to(device)\n",
    "\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "\n",
    "            text = tokenizer.decode(output_ids[0])\n",
    "            results.append((prompt, text))\n",
    "    return results\n",
    "\n",
    "# 1) 사용할 체크포인트 경로 선택\n",
    "ckpt_path = \"checkpoint_final.pt\"        \n",
    "\n",
    "# 2) 모델 + 토크나이저 로드\n",
    "model, tokenizer, device = load_model_and_tokenizer(ckpt_path, CONFIG)\n",
    "\n",
    "# 3) 프롬프트 정의\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In the beginning,\",\n",
    "    \"Machine learning is\",\n",
    "]\n",
    "\n",
    "# 4) 문장 생성\n",
    "samples = generate_from_prompts(model, tokenizer, device, prompts,\n",
    "                                max_new_tokens=50,\n",
    "                                temperature=0.8,\n",
    "                                top_k=50)\n",
    "\n",
    "# 5) 출력\n",
    "for i, (prompt, text) in enumerate(samples):\n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(f\"[Prompt] {prompt}\")\n",
    "    print(f\"[Generated] {text}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5154cd8b-102a-4ec6-b27a-16941e4ebb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 0 ===\n",
      "[Prompt] South Korea is\n",
      "[Generated] South Korea is a strong tourist occurrence of nuclear airfields , but the <unk> ( April 26 , 1972 ) has been built in the region . The Romanian Land Forces created the National Historic Land Forces in the National Register of Historic Places . \n",
      "<|endoftext|>\n",
      "\n",
      "=== Sample 1 ===\n",
      "[Prompt] In the beginning,\n",
      "[Generated] In the beginning, Zealand , the Germans was used as a brief armed vessel in the region . \n",
      "<|endoftext|>\n",
      "\n",
      "=== Sample 2 ===\n",
      "[Prompt] Germany is\n",
      "[Generated] Germany is a short @-@ down by the NS @-@ 10 . The song has a low @-@ long @-@ hand , and the Sun @-@ off hand , and his second hand , and the color of the eyes , and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"South Korea is\",\n",
    "    \"In the beginning,\",\n",
    "    \"Germany is\",\n",
    "]\n",
    "\n",
    "# 4) 문장 생성\n",
    "samples = generate_from_prompts(model, tokenizer, device, prompts,\n",
    "                                max_new_tokens=50,\n",
    "                                temperature=0.8,\n",
    "                                top_k=50)\n",
    "\n",
    "# 5) 출력\n",
    "for i, (prompt, text) in enumerate(samples):\n",
    "    print(f\"=== Sample {i} ===\")\n",
    "    print(f\"[Prompt] {prompt}\")\n",
    "    print(f\"[Generated] {text}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (3.12 cu121)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
